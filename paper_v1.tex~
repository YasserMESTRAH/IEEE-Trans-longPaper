%\documentclass[a4paper,peerreview,draftcls]{IEEEtran}
%\documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
\documentclass[a4paper]{IEEEtran}
\usepackage{graphicx}
\usepackage[cmex10]{amsmath} 
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{subcaption}
\usepackage{float}
\usepackage{mwe}
\usepackage{tabularx,booktabs}
%\usepackage{comment}
\usepackage{theorem}
\newcolumntype{C}{>{\centering\arraybackslash}X} % centered version of "X" type
\setlength{\extrarowheight}{1pt}
\usepackage{lipsum}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor}
\usepackage{multirow}
\makeatother
\graphicspath{{graphics-paper/}}

\definecolor{Gray}{gray}{0.6}
\definecolor{lightGray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\title{A blind LLR estimation for impulsive noise}
\author{Yasser Mestrah, Anne Savard,  Alban Goupil, Guillaume Gell\'{e} and Laurent Clavier}
 %\thanks{
 % *CReSTIC, University of Reims Champagne-Ardenne, France\\
 % \(\dagger\)IEMN UMR CNRS 8520, IRCICA CNRS 3024\\
 % \(\ddagger\)Institute Mines T\'{e}l\'{e}com, T\'{e}l\'{e}com Lille 1,France 
 %}}


\newtheorem{theorem}{Theorem} 
\newtheorem{proposition}{Proposition} 
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\begin{document}
 \maketitle



%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

\begin{abstract}
  Many decoding schemes rely on the log-likelihood ratio (LLR). Its derivation rely on the knowledge of the noise distribution. In dense and hetrogeneous network settings, this knowledge can be difficult. Besides, when interference exhibits an impulsive behaviour, the
  LLR becomes highly non-linear and, consequently, computationally
  prohibitive. In this paper, we propose to directly estimate the LLR, without relying on the interference plus noise knowledge. We propose to select the LLR in a parametric family of functions, flexible enough to be able to represent many different communication contexts. It also allows to limit the number of parameters to be estimated. 
  Furthermore, we propose an unsupervised estimation approach which do not necessitate a training sequence. Our estimation method is shown to be efficient and, if the set of parametric function is well chosen, the receiver exhibits a near-optimal performance.
\end{abstract}

\begin{IEEEkeywords}
  LLR estimation, impulsive noise,
 unsupervised learning.
\end{IEEEkeywords}


\section{Introduction}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

5G will have to deal with dense and heterogeneous networks. In such
situations, interference may exhibit a dynamic behaviour
\cite{PM-2010-2,P-2010,Egan2017} where the Gaussian assumption is no
longer suited.  In order to establish reliable and efficient
communications, one needs to take into account the impulsive nature of
the interference while designing the receivers. Indeed, traditional
linear receivers under impulsive noise were shown not to be robust
enough leading to a dramatic performance degradation \cite{PM-2010-1}.
Several papers proposed ways to overcome the presence of non-Gaussian
interference by using different metrics to make the decision, e.g. a
robust metric mixing euclidean distance and erasure \cite{DG-2009},
$p$-norm \cite{WL-2012}, Hubber metric \cite{T-2005}... Nevertheless,
the approaches are designed for a specific noise model and their
robustness against a model mismatch is not ensured. The choice of a
more universal solution that can be used for various impulsive noise
is thus salutary.

Many receivers rely on the Likelihood. In the binary case, this can be
captured through the log-likelihood ratio (LLR). This is very
attractive when noise is Gaussian because it leads to a linear
receiver, straightforward to implement. However, when rare events with
large amplitudes (impulsive noise) arise, the LLR becomes a non-linear
function. Its implementation is complex and highly depends on the
noise distribution. Consequently, to obtain the optimal receiver, the
noise distribution has to be known and if it falls in a parametric
family, the parameters have to be estimated. In addition to the
complex implementation of a non-linear function, we can identify two
sources of performance degradation: a noise model mismatch and errors
in parameters estimation.

In this paper we propose to address these questions. Our solutions resides in three steps:
\begin{enumerate}
\item First, we propose to approximate the LLR by a function which
  will be chosen in a family of parametric functions. If the family is
  large enough, this allows to adapt to many different types of noises
  but it does not rely on a noise assumption. Besides, if we consider
  a family defined by a limited number of parameters and easy to
  implement, it reduced both the estimation and implementation
  complexities.
\item Second, we propose a robust way to estimate the parameters of
  the LLR approximation. The solution is based on a mutual information
  maximization.
\item Finally, we propose a blind estimation solution. It avoids the
  need of training data that reduce the useful information rate. It
  also allows to take benefit from the whole data sequence to improve
  the accuracy of the estimation.
\end{enumerate}

Our work can be seen as a generalization of some previous works that
dealt with an approximation of the LLR function. If the soft limiter
and the hole puncher are probably the best-known solutions, we
previously proposed the approximation function:
$f(x)=\text{sign}(x)\text{min}(a|x|,b/|x|)$ \cite{VALG-2014}. The
family $f(x)=x/(ax^2+b)$ was proposed in \cite{YRL-2014} and linear by
parts functions in \cite{CRB-2017}. We will not focus in this paper on
the best family choice but on a generic way to estimate the
approximation parameters in an unsupervised manner.


The remaining of the paper is organized as follow. Once the system
model and some background material are given in
section~\ref{section:systemModel}, section~\ref{section:parameterOpt}
undertakes the parameters’ optimization in supervised and unsupervised
ways. Section~\ref{section:ParameterInvestigations} presents numerical
simulations under alpha-stable noise. Thereafter
section~\ref{section:robustness} investigates the robustness and
adaptability of our receiver in various noise models and, finally,
section~\ref{section:conclusion} concludes the paper.

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%

%\begin{comment}

% Another approach for designing the receiver is to rely on an
% approximation of the LLR. We propose to approximate the LLR with the
% function: $f(x)=\text{sign}(x)\text{min}(a|x|,b/|x|)$. Other families
% of function have also been proposed like $Y/(aY^2+b)$ in
% \cite{YRL-2014} or a function linear by parts in \cite{CRB-2017}. Our
% proposal is more accurate and has more flexibility to adapt to other
% impulsive types, in particular, the non-impulsive
% scenarios. Consequently, we complete and extend the framework we
% initially introduced in \cite{VALG-2014}. This solution is attractive
% because of its simplicity and its easiness of implementation. Our
% proposal is to directly design the shape of the LLR without the need
% to assume the distribution of the noise and interference samples: we
% encompass all types of sub-exponential or heavy-tailed distributions,
% which are good representative of impulsive noise, in this
% framework. Our solution is valid for a larger family of noise
% distributions. One difficulty we address in this paper is the
% estimation of the two parameters $a$ and $b$. We use an optimization
% objective inspired by information theory. If a training sequence can
% be used, it generates an important overload and can miss some of the
% rare events that would be present in the payload but not in the
% training sequence. To overcome these issues we propose an unsupervised
% approach that makes use of all the received information data to
% estimate the needed parameters, moreover, it can be implemented to any
% approximation function.

% The main contributions of the paper are as follows:
% \begin{itemize}
% \item We complete and extend the approximated LLR function we proposed
%   in \cite{VALG-2014}. We introduce an unsupervised estimation of the
%   required parameters.

% \item We analyze the performance of our proposal and compare it to the
%   ideal receiver and to a supervised estimation approach. Our proposal
%   shows to be efficient and robust in different types of noises and do
%   not degrade the performance of the receiver when the noise is purely
%   Gaussian.
  
% \item We also show that under the supervised approach the training
%   sequence has to be long to ensure a good performance.
  
% \item We show the robustness and reliability of our solution against
%   parameter estimation errors and various noise models.
% \end{itemize}

% The remaining of the paper is organized as follow. Once the system
% model and some background material are given in
% section~\ref{section:systemModel}, section~\ref{section:parameterOpt}
% undertakes the parameters’ optimization in supervised and unsupervised
% ways. Section~\ref{section:ParameterInvestigations} presents numerical
% simulations under alpha-stable noise. Thereafter
% section~\ref{section:robustness} investigates the robustness and
% adaptability of our receiver in various noise models and, finally,
% section~\ref{section:conclusion} concludes the paper.


% %----------------------------------------------------------------------%
% %----------------------------------------------------------------------%
% %----------------------------------------------------------------------%

% LLR important because
% capacity: function of LLR
% BER: function of LLR
% impulsive noise: no closed form expression for LLR
% need a blind estimation
% \end{comment}

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\section{LLR approximation and estimation}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\subsection{System model and LLR}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
Let us consider the transmission of a binary message
$X \in \left \{+1;-1 \right \}$ with equal probability. The received
signal $Y$ can be written as $Y = X + N$, the addition of the source
signal $X$ and an interference term (interference limited regime) $N$,
where $N$ is assumed to be independent of $X$.  Throughout this paper,
we focus on a transmission over a memory-less binary input
symmetric-output (MBISO) channel. Such a channel can be described by
its conditional pdf \(f_{Y|X}(y|x)\) with
$f_{Y|X}(y|x=+1)= f_{Y|X}(-y|x=-1)$.

By considering an additive noise MBISO channel, the LLR of the binary
channel input $X$ associated with the channel output $Y$ is given by:
\begin{equation}
   \label{eq:2}
   \Lambda(y)=\log\frac{\Pr(Y=y\vert X=+1)}{\Pr(Y=y\vert X=-1)}= \log\frac{f_N(y-1)}{f_N(y+1)}
 \end{equation}
 where $f_N(\cdot)$ is the pdf of the noise $N$.

 In various situations, obtaining the LLR can be computationally very
 costly; for instance over additive symmetric $\alpha$-stable
 (S$\alpha$S) noise (AIS$\alpha$SN) channel for which no closed form
 of the LLR can be obtained \cite{PM-2010-1}; or with Middleton type
 noises which involve infinite series \cite{Middleton1977}. However,
 many decoding solutions rely on the soft information given by the
 LLR, like the Belief Propagation algorithm used to decode LDPC
 codes. In such cases, obtaining an easy to implement approximation of
 the LLR can be crucial. This can be done through the use of simple
 approximations. It has been shown in several papers
 \cite{VALG-2014,YRL-2014,CRB-2017} and it allows good performance in
 terms of BER with a drastic reduction in the computation
 complexity. In order to narrow the search space of the best
 approximation, the functions that we look for belong to a parametric
 family $L_{\theta}$. Our goal is to find a generic way to estimate
 $\theta$. For this we first choose a criteria design. Because there
 is not an easy analytic link between the LLR and the BER or PER (that
 is what we aim at minimizing), we propose to use the capacity of the
 MBISO channel that can be expressed as a function of the LLR.


 \subsection{Capacity, LLR and optimization problem}

 The capacity of a MBISO channel is given by the mutual information
 (MI) between the input $X$ and the output $Y$ as $C(X,Y)=I(X, Y)$,
 such that $\Pr(X=1)=\Pr(X=-1)=1/2$. In this case, the capacity
 \cite{RU-2008} can be given as:
\begin{equation}
   \label{eq:11}
   C_L(X,Y) = 1- \mathbb{E}\left[\log_2\left(1+e^{-X.%\,
         \Lambda(Y)}\right)\right].
\end{equation}


If now we modify the LLR calculation by using the approximation
function $L_{\theta}$, the mutual information is modified. We can
calculate the capacity of a MBISO channel that includes the
approximated LLR by:
\begin{align}
  C_{L_{\theta}}(X,Y)& = 1-\mathbb{E}\left[\log_2(1+e^{-X.L_{\theta}(Y)})\right]\label{eq:12}\\
                     &\approx 1-\lim_{N\to\infty}\frac{1}{N}\sum_{n=1}^{N}\log_2\left(1+e^{-x_nL_{\theta}(y_n)}\right),\label{eq:12b}
  \end{align}
  where in \eqref{eq:12b} we estimate the expectation based on samples
  $x_n$ and $y_n$ that represent the input and the output of the
  channel respectively. Authors in \cite{YA-2009} proved that
  \eqref{eq:12} is maximized if $L_{\theta}=\Lambda$, leading to
  $C_{L_{\theta}}= C_L$.

  Maximizing $C_{L_{\theta}}$ in \eqref{eq:12b} over the possible
  choices of $\theta$ will allow us to find the approximation in the
  considered family that maximizing the mutual information and,
  consequently, should be a good choice for our decoding
  algorithm. \textcolor{blue}{\textsc{Pas très sur de comment
      argumenter çà... Faut-il le faire?}}.  Our optimization problem
  is therefore given as:
  \begin{equation}
  \label{eq:15}
  \begin{aligned}
    \theta^* &= \arg\max_{\theta}C_{L_{\theta}}(X,Y).\\
    &=
    \arg\min_{\theta}\underbrace{\frac{1}{N}\sum_{n=1}^{N}\log_2\left(1+e^{-x_nL_{\theta}(y_n)}\right)}_\text{{$f_{opt}(\theta)$}}.
  \end{aligned}
\end{equation}
Equivalently, the maximization of the capacity is achieved by
minimizing
$f_{opt}(\cdot)$. % which will be tackled in our implementation via a
                  % simplex method based algorithm \cite{NM-1965}.

Our next question is then to know if our problem has a solution and if
that solution will be easy to find
  
  
% In the next subsection, we state our convex optimization problem.

%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\subsection{Convexity}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

In the following we consider linear piece-wise functions. %such as
%  \begin{equation}
%      L_a(y)=ay,
% \end{equation}
% \begin{equation}
%   L_{a,b}(y)=\begin{cases}
%    ay &\text{ if } y^2\leq b/a\\
%    b/y &\text{ else}
%   \end{cases}
%  \end{equation} 
%  or 
%  \begin{equation}
%      L_{a,b,c}=.....
%  \end{equation}
\begin{proposition}
  If the estimate LLR $L_{\theta}$ is a linear piece-wise function of
  $\theta$, then the optimization problem is convex and there exists a
  unique optimal value $\theta^*$ that minimizes $f_{opt}$, hence that
  maximizes $C_{L_{\theta}}$.
\end{proposition}

\begin{IEEEproof}
  For clarity reasons, we present the proof based on
  $L_{\theta}=L_{a,b}$ defined in \eqref{eq:12c}, but the result can
  be easily extended to any linear piece-wise function.
\begin{equation}
  L_{a,b}(y)=\begin{cases}
    ay &\text{ if } y^2\leq b/a\\
    b/y &\text{ else}
   \end{cases}
   \label{eq:12c}
\end{equation} 

Let us consider the function $f(a,b)=\log_2(1+e^{-x.L_{ab}(y)})$. Note
that $f_{opt}$ is given as a weighted sum of such functions thus if
one can prove that $f$ is a convex function then $f_{opt}(a,b)$ is
also convex.
 
The Hessian of the function $f$ is given as

\begin{equation}
  \bigtriangledown^2f=
   \begin{cases}
     \begin{bmatrix}
       \frac{y^2x^2e^{axy}}{(1+e^{axy})^2} & 0 \\
       0 & 0
     \end{bmatrix} & \text{ if } y^2\leq b/a\\
     \begin{bmatrix}
       0 & 0\\
       0 & \frac{x^2e^{bx/y}}{y^2(1+e^{bx/y})^2}
     \end{bmatrix} & \text{ else}
   \end{cases}
 \end{equation}

 To show that our problem is convex, one need to show that
 $\forall z \in \mathbb{R}^2$, $z^T\bigtriangledown^2f z\geq 0$.  Let
 $z=[z_1,z_2]^T\in \mathbb{R}^2$.
 \begin{align}
   z^T \bigtriangledown^2f z &= \begin{cases}
     z_1^2 \frac{y^2x^2e^{axy}}{(1+e^{axy})^2} &\text{ if } y^2\leq b/a\\
     z_2^2 \frac{x^2e^{bx/y}}{y^2(1+e^{bx/y})^2} &\text{ else }
   \end{cases}\\
                             &\geq 0 \, \forall z\in \mathbb{R}^2
 \end{align}
 Thus, the function $f_{opt}$ is convex leading to the existence of a
 unique optimal point $\theta^*=[a^*,b^*]$ that maximizes the capacity
 under the approximated LLR $L_{\theta}$.

\end{IEEEproof}

A consequence is that we can implement the optimization problem via a
simplex method based algorithm \cite{NM-1965}.

\begin{proposition}
  If the estimate LLR $L_{\theta}$ is a non-linear function of
  $\theta$, then we cannot guarantee the convexity of the optimization
  problem, thus, a global search method is used to find a unique
  optimal value $\theta^*$ that minimizes $f_{opt}$, hence maximizes
  $C_{L_{\theta}}$.
\end{proposition}

\begin{IEEEproof}
  For clarity reasons, we present the proof based on non linear
  approximation $L_{\theta}=L_{a,b}$ defined in \eqref{eq:12c}.
\begin{equation}
  L_{a,b}(y)=\frac{y}{ay^2+b}
   \label{eq:12c}
\end{equation} 

Let us consider the function $f(a,b)=\log_2(1+e^{-x.\frac{y}{ay^2+b}})$. Note
that $f_{opt}$ is given as a weighted sum of such functions thus if
one can prove that $f$ is a convex function then $f_{opt}(a,b)$ is
also convex.
 
The Hessian of the function $f$ is given as

\begin{equation}
  \bigtriangledown^2f= xy\mathbf{K}
     \begin{bmatrix}
      y^4 & y^2 \\
       y^2 & 1
     \end{bmatrix} 
   \end{equation}

   where
   \begin{align}xy\mathbf{K} &= \dfrac{\varphi y^2x^2-2xy\left(\varphi+1\right)\left(ay^2+b\right)}{\ln\left(2\right)\left(\varphi+1\right)^2\left(ay^2+b\right)^4}\\
     \varphi&=\mathrm{e}^\frac{y}{ay^2+b}
   \end{align}

   
 To show that our problem is convex, one need to show that
 $\forall z \in \mathbb{R}^2$, $z^T\bigtriangledown^2f z\geq 0$.  Let
 $z=[z_1,z_2]^T\in \mathbb{R}^2$.
 \begin{align}
   z^T \bigtriangledown^2f z &= xy\mathbf{K}(z_1y^2+z_2)^2 &\geq 0 \, \forall z\in \mathbb{R}^2
 \end{align}
 Thus, the function $f_{opt}$ cannot be guaranteed to be positive
 leading to the existence of a several local solutions.

\end{IEEEproof}


%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\subsection{Unsupervised optimization.}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%

To solve \eqref{eq:15}, we need a received sequence $y_n$ as well as
the corresponding transmitted one $x_n$. This is usually obtained
thanks to the use of a training sequence \cite{VALG-2016}. However,
the consequence is an increase in the signaling and a decrease in the
useful data rate.
 
% The remaining question is thus how to optimize $\theta=[a,b]$ in
% order to maximize $C_{L_{a,b}}(X,Y)$. This optimization can be
% performed in a supervised manner, as proposed previously in
% \textcolor{red}{REF} or as proposed in the following in a blind
% manner.

%A major drawback of the supervised approach relies on the necessity
% of the learning sequence, especially for varying channels, since for
% each realization of the channel, a learning sequence has to be
% used. The task is especially difficult with impulsive noise that
% makes the estimation step more complex and requires longer training
% sequence to observe enough rare events. In such a way, what we
% learned from the learning sequence may not match the real condition
% of the message leading to a severe performance degradation.

Unsupervised optimization is thus attractive since it does not imply
any overload. Besides, a good aspect of having such an unsupervised
approach is that we optimize the approximation function directly from
the sequence that we are going to decode. In other words, the noise
impacting the training phase and the decoding phase will be the same
ensuring the best knowledge of the actual channel
state. %Moreover, our proposed method features a simple and fast
       %implementation.

The idea behind our approach is to obtain a noise sequence $N$ from
the received sequence $Y$. To do this, we use a sign-detector
(sign$(x)=x/\left|x\right|$)
\begin{equation}
   \label{eq:16}
   \widetilde{N}= Y-\text{sign}(Y).
\end{equation}
where $\widetilde{N}$ is the generated noise sequence.  Using
$\widetilde{N}$, a new channel can be simulated with a known input
$\widetilde{X}$ as shown in \figurename~\ref{fig:5}. More precisely,
$\widetilde{X}$ is an i.i.d. BPSK random variable that is independent
of $\widetilde{N}$. The channel output $\widetilde{Y}$ is obtained by
adding $\widetilde{X}$ and $\widetilde{N}$ and the optimization
parameter $\theta$ can be estimated based on \eqref{eq:15} but with
the newly generated input and
output: %y maximizing the capacity between the input sequence
        %$\widetilde{X}$ and the approximated demapper of the received
        %sequence over the new channel $L_{\theta}(\widetilde{Y})$
        %where $\widetilde{Y}=\widetilde{X}+\widetilde{N}$ as
\begin{equation}
  \label{eq:17}
  \theta^*=\arg\max_\theta C_{L_{\theta}}(\widetilde{X},\widetilde{Y}).
\end{equation}

\begin{figure}[htbp]
  \centering \includegraphics[width=1.0\linewidth]{fig-5}
  \caption{Unsupervised LLR demapper.}
  \label{fig:5}
\end{figure}

Once the optimal parameters $\theta^*$ are obtained, the LLR will be
approximated by $L_{\theta^*}(Y)$.


% In order to be efficient, the optimal values for $\theta$ obtained
% under the supervised and the unsupervised optimization must be close
% to each other.

% ----------------------------------------------------------------------%
% ----------------------------------------------------------------------%%----------------------------------------------------------------------%
% ----------------------------------------------------------------------%

%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\section{Application to LDPC coding used over additive impulsive noise}
\label{section:ldpc}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

\subsection{Simulation setup}

\subsubsection{Source}

To validate our approach we use a Low-Density Parity Check (LDPC) code
and a belief propagation (BP) decoding algorithm. This case is
well-suited to our proposal because the LLR have to be estimated and
fed to the BP algorithm.

The binary message $X$ is encoded using a regular (3,6) LDPC code of
length 20000. LDPC codes introduced by Gallager \cite{G-1963} and
rediscovered by Mackay \cite{MN-1996} are powerful block codes due to
their near capacity performance under Gaussian noise. They are widely
used in various standards such as DVB-S2 and
WiMAX. %Usually LDPC codes are decoded using the Belief Propagation
       %(BP) algorithm whose inputs are the LLRs.
Thanks to the sparseness of the parity check matrix of the LDPC codes,
the BP algorithm exhibits a quasi-linear complexity as the code length
growth.

\subsubsection{Noise}

Non Gaussian noises can arise in networks like it has been shown many
times. In the following we assume that the additive noise impacting
the transmission exhibits an impulsive nature. In a first step, we
will use Symmetric $\alpha$-stable ($S\alpha S$) distributions to
model this impulsive interference, since the heavy tail property of
their pdf has been shown to coincide with the impulsive nature of
network interference in various environment types
\cite{NHJ-2008,MPL-2009,HLNFP-2010,E-1992} .  Unfortunately, in
general, no closed-form expression of its pdf exists, which prevents
the extraction of a simple metric based on the noise pdf in the
decoding algorithm. This is precisely a case that can be solved by our
proposal.

We also want to be model agnostic. Consequently we will study the
behavior of our approach with other classical noise models: a simple
Gaussian noise, a Middleton class A and an $\epsilon$-contaminated.


\subsubsection{LLR approximation}

If the noise is Gaussian, the decoder's inputs (LLR) are given as a
linear function of the received signal $Y$. The family of function
should then simply be:
\begin{equation}
    L_{a}(y)=ay
\end{equation}
and $\theta$ is a single parameter, the slope $a$ of the linear
function.  The optimal $a$ in additive white Gaussian noise channel
only depends on the noise variance as $L_a(y)=\frac{2}{\sigma_N^2}y$.


Nevertheless, using only a linear scaling whose slope depends on the
additive noise variance leads to severe performance loss as soon as
noise is
impulsive. %, as we will see in
           %Section~\ref{section:ParameterInvestigations} and
           %Section~\ref{section:robustness}.
This performance loss occurs because with this linear scaling, large
values in $Y$ result into large LLR. However, under impulsive noise,
large values in $Y$ are more likely due to an impulsive event so that
the LLR should be small, meaning a poorly reliable sample due to the
presence of a large noise sample.

\figurename~\ref{fig:3} lightens the non-linearity of the LLR function
for the channel output $Y$ when the noise is $\alpha$-stable. Even if
\figurename~\ref{fig:3} delineates a specific noise model, the overall
appearance of the LLR exhibits a similar behavior when noise is
impulsive.
\begin{figure}[htpb]
  \centering \includegraphics[width=1.0\linewidth]{fig-3}
  \caption{LLR demapper for \(\alpha=1.4\), \(\gamma=0.5\), and its
    approximation.}
  \label{fig:3}
\end{figure}
At a first look, two different parts in the LLR can be observed: a
first one when $y$ is close to zero and another one when $y$ becomes
large enough.  When $y$ is close to zero, the LLR is almost linear,
whereas when $y$ is large enough, the LLR presents a power-law
decrease. The linear region spreads with the decrease of the noise
impulsiveness until reaching the limit (only the linear part exists),
when the noise is Gaussian. Several approximations taking into account
these two parts have been proposed
\cite{VALG-2014,SJD-1994,HALG-2013,SME-2012}. For illustration
purposes, we choose the LLR approximation family $L_{a,b}$ given in
\eqref{eq:12c}.


%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

\subsection{Estimation in Additive S$\alpha$S Noise} %Comparison between $\theta^*$ obtained under supervised and unsupervised optimization over Additive S$\alpha$S Noise (AS$\alpha$SN) channels and link with BER}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

\subsubsection{Analysis}

In a first step, we investigate the shape of the function
$f_{\text{opt}}(a, b)$ in (\ref{eq:15}).  In this paper we present the
obtained results for a highly impulsive noise when $\alpha=1.4$, but
the same observations and conclusions can be made for higher $\alpha$.
In \figurename~\ref{fig:12}, we draw a 3D and contour plot that
delineates the different $f_{opt}$ level values for different
combinations of $a$ and $b$.
\begin{figure*}[htbp]
\begin{multicols}{3}
  \includegraphics[height=0.3\textheight,width=1.05\linewidth]{fig-12a}\par
  \includegraphics[height=0.3\textheight,width=1.05\linewidth]{fig-12b}\par
  \includegraphics[height=0.3\textheight,width=1.05\linewidth]{fig-12c}\par
\end{multicols}
\caption{Behaviour study of the $f_{opt}$ function, as a function of
  parameters a and b for different values of $\gamma$, under highly
  impulsive $S\alpha S$ noise with $\alpha =1.4$.}
\label{fig:12}
\end{figure*}
We verify the convexity of the function.  We also notice that the
function is rather flat, the convergence should be easy but the
estimation steps could be quite sensitive to noise. The optimization
procedure could then have a significant impact and the system could be
sensitive to the length of the training sequence. Using the whole data
set in an unsupervised approach can then be a source of robustness.

In \figurename~\ref{fig:13} we illustrate the link between the
function $f_{\text{opt}}(a, b)$ and the BER obtained after
decoding. We show the contour plot of $f_{\text{opt}}(a, b)$, meaning
all values inside the circle corresponds to a smal value of the
function. We also show the BER contour plots. The region matches and
all the values minimizing $f_{\text{opt}}(a, b)$ give a BER under
$10^{-4}$.  These pictures conforts us in the choice of the criteria
to find $\theta$.
\begin{figure}[h!]
  \centering
  \includegraphics[height=0.238\textheight,width=1.01\linewidth]{fig-13}
  \caption{BER comparison as a function of $a$ and $b$ parameters with
    $\gamma = 0.45$ and $\alpha=1.4$ under the supervised
    approximation.}
  \label{fig:13}
\end{figure}

The enclosed minimal level of $f_{opt}$
value matches the low BER levels as shown in \figurename~\ref{fig:13},
thus, obtaining the optimal value of $f_{opt}$ ideally, will result
with the optimal BER.


 %The first goal of
%this study is to show numerically that an optimized BER will arise as
%a result of optimizing $f_{\text{opt}}$. 
%
%Nevertheless, due to the impulsive context, both the function
%$f_{\text{opt}}$ and the BER are sensitive to estimation errors. An
%estimation error in $a$ and $b$ can lead to a significant degradation
%of the BER, making the estimation step difficult, especially when the
%learning sequence will be shortened, as shown in the next
%section. Consequently, this underlines the benefit of our unsupervised
%approach that allows to perform the estimation on the whole sequence
%rather than on added bits, carrying no information. 

\bigskip

\subsubsection{Estimation performance}

\figurename~\ref{fig:7}, respectively \figurename~\ref{fig:8},
compares the evolution of the mean and variance of the estimated
parameter $a$, respectively $b$, as a function of the dispersion
$\gamma$ of a $S\alpha S$ noise with $\alpha=1.4$ under supervised and
unsupervised optimization. For each noise dispersion, we ran 100
experiments. For the supervised case, we use a learning sequence of
20000 samples to estimate $a$ and $b$. This allows to have a good idea of the results with a very small estimation error but, in a practical setting, such a long training sequence is not reasonable and additional errors can be expected.
\begin{figure}[h!]
  \centering \includegraphics[width=1.0\linewidth]{fig-7}
  \caption{Comparison of the mean and standard deviation evolution for
    parameter $a$ as a function of the dispersion $\gamma$ of a
    S$\alpha$S noise with $\alpha=1.4$ for the supervised and
    unsupervised optimization.}
  \label{fig:7}
\end{figure}
\begin{figure}[h!]
  \centering \includegraphics[width=1.0\linewidth]{fig-8}
  \caption{Comparison of the mean and standard deviation evolution for
    the parameter $b$ as a function of the dispersion $\gamma$ of a
    S$\alpha$S noise with $\alpha=1.4$ for the supervised and
    unsupervised optimization.}
  \label{fig:8}
\end{figure}

In \figurename~\ref{fig:7}, the gap between the obtained
values for parameter $a$ under supervised and unsupervised
optimization is small. In \figurename~\ref{fig:8}, the one obtained for $b$ is
significantly larger. This difference can be explained because $b$ mainly depends on large noise samples which are rare events. Consequently, its estimation is less accurate. Nevertheless, the unsupervised approach generates a bias. 

We can however expect that the error on $b$ will have a limmited impact in terms of BER performance. Indeed, as shown in \figurename~\ref{fig:13} when $\gamma=0.45$, both $a$ and $b$ estimated mean values under the supervised and unsupervised approach fall in the small BER region. 
Besides, the small variance of the estimation ensures that most of the estimated values will fall in the same reagion.
%The error bars indicate the small impact of different
%realizations under supervised and unsupervised approaches,
%while shortening the learning sequence under the supervised approach
%will induce large error values as highlighted in Table~\ref{tab:a},
%which indicates the significant impact of different realizations being
%more likely to fall outside the optimal BER region. For each noise
%dispersion $\gamma$, we ran 1000 experiments.

\bigskip

To complete the study we present in Table \ref{tab:a} the influence of the training sequence length. The variance in $a$ and $b$ gets large when the training sequence gets short. This will probably influence the performance of the system and degrade the BER.
\begin{table*}
%\label{my-label}
\begin{tabularx}{\textwidth}{ccc||CCCC||}
  \toprule \multicolumn{3}{c||}{} & \cellcolor{LightCyan}$\mu_{a}$ & \cellcolor{LightCyan}$\sigma_{a}$& \cellcolor{LightCyan}$\mu_{b}$ & \cellcolor{LightCyan} {}$\sigma_{b}$ \\
  %\midrule \multirow{8}{4em}{$\alpha=1.4$}&
 % \multirow{4}{4em}{$\gamma=0.43$}
 % &$\text{Unsupervised}$          & 3.80      & 0.07        &  4.07                         & 0.08  \\
 % &  &$\text{Sup}_{\text{LS=20000}}$  & 3.73    & 0.07      & 5.10                          & 0.12 \\
  %&  &$\text{Sup}_{\text{LS=1200}}$  &  3.77   &  0.35      &  5.18   &  0.58\\
 % &  &$\text{Sup}_{\text{LS=900}}$   &  3.77   &   0.40     &  5.28   & 0.71\\
 % \addlinespace &\multirow{4}{4em}{$\gamma=0.45$}
 % &$\text{Unsupervised}$       & 3.66     & 0.07      & 3.98                         & 0.07  \\
 % &  &$\text{Sup}_{\text{LS=20000}}$  & 3.57    & 0.07      & 5.06                          & 0.13 \\
 % &  &$\text{Sup}_{\text{LS=1200}}$   &  3.60   & 0.32      &   5.15   &  0.58\\
 % &  &$\text{Sup}_{\text{LS=900}}$    &  3.61   &   0.37        &  5.16        & 0.66\\
  \addlinespace \midrule \multirow{8}{4em}{$\alpha=1.8$}&
  \multirow{4}{4em}{$\gamma=0.53$}
  &$\text{Unsupervised}$       & 3.43    & 0.06                                & 5.73          & 0.15 \\
  &  &$\text{Sup}_{\text{LS=20000}}$ & 3.25    & 0.05                             & 7.59       & 0.28 \\
  &  &$\text{Sup}_{\text{LS=1200}}$  &  3.27    &   0.24     & \cellcolor{lightGray}  8.50       & \cellcolor{Gray} 14.48\\
  &  &$\text{Sup}_{\text{LS=900}}$   &  3.27    &   0.27          & \cellcolor{Gray}  11.72      & \cellcolor{Gray} 46.15\\
  \addlinespace &\multirow{4}{4em}{$\gamma=0.55$}
  &$\text{Unsupervised}$      & 3.23    & 0.05                                  & 5.61          & 0.14 \\
  &  &$\text{Sup}_{\text{LS=20000}}$ & 3.05    & 0.05                              & 7.62      & 0.28 \\
  &  &$\text{Sup}_{\text{LS=1200}}$  &  3.07   &0.22         & 7.97      & \cellcolor{lightGray} 1.54\\
  &  &$\text{Sup}_{\text{LS=900}}$   &  3.07   &  0.26             &  \cellcolor{Gray} 10.73    & \cellcolor{Gray} 30.41\\
  \addlinespace\bottomrule
\end{tabularx}
\caption{Comparison of the mean and standard deviation evolution for
  the parameters ($a, b$) as a function of the dispersion $\gamma$ of
  a S$\alpha$S noise with $\alpha=1.8$ for the
  supervised with different learning sequences and unsupervised
  optimization.}
\label{tab:a}
\end{table*}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\subsection{BER performance}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

\subsubsection{S$\alpha$S additive noise channel}

Once our demapper is tuned with the estimated value $\theta$, it is
used as a front-end to the 20000 bits long regular (3,6) LDPC decoder
using the BP algorithm over an additive impulsive $S\alpha S$
noise. We study a highly impulsive situation with $\alpha= 1.4$ and a
more moderate case with $\alpha=1.8$.

\figurename~\ref{fig:10} and \figurename~\ref{fig:11} present the
obtained BER for $\alpha=1.4$ and $\alpha=1.8$ respectively, as a
function of the dispersion $\gamma$ of the $\alpha$-stable
noise\footnote{In case of an impulsive environment with $\alpha < 2$,
  the second-order moment of a stable variable is infinite
  \cite[Theorem 3]{NHJ-2008}, making the conventional noise power
  measurement infinite. Accordingly, we present our simulation results
  as a function of the dispersion parameter $\gamma$, which is used as
  a measurement of the strength of the $\alpha$-stable noise.}.  In
both cases, we compare the BER obtained via demapping function, either
in a blind or supervised manner, to the BER obtained with the true LLR
computed via numerical integration. For each channel set, we use a
learning sequence of length (1200 or 20000) to optimize $\theta$ in
the supervised case; the long training sequence (20000) allows to
assess the optimal performance of the supervised estimation, the
shorter one (1200) allows to evaluate the loss due to estimation with
more realistic training sequences.
\begin{figure}[htbp]
\centering \includegraphics[width=1.00\linewidth]{fig-10}
\caption{Evolution comparison of the BER as a function of the
  dispersion $\gamma$ of a S$\alpha$S noise in poorly impulsive
  environment with $\alpha=1.8$, between the supervised with different
  learning sequence sizes, unsupervised, Gaussian designed LLR
  approximations and the LLR obtained by numerical integration.}
\label{fig:10}
\end{figure}
\begin{figure}[htbp]
  \centering \includegraphics[width=1.00\linewidth]{fig-11}
  \caption{Evolution comparison of the BER as a function of the
    dispersion $\gamma$ of a S$\alpha$S noise in highly impulsive
    environment with $\alpha=1.4$, between the supervised with
    different learning sequence sizes, unsupervised, Gaussian designed
    LLR approximations and the LLR obtained by numerical integration.}
  \label{fig:11}
\end{figure}

First, we note that the estimation with a long training sequence gives
performance close to the optimal LLR which shows the good behavior of
our demapping function. Moreover, the unsupervised approach does not
perform as well as the supervised one with long training sequence but
the gap is not so large and the gain in comparison to a linear
receiver is enormous. However, when the training sequence is
shortened, the supervised estimation degrades and the performance of
the blind approach is then much better.

%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\subsection{Robustness study of the proposed LLR approximation over impulsive channels}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\subsubsection{Other additive impulsive noise channels}

%\textcolor{red}{$L_{ab}$ idea for S$\alpha$S noise, what if if other noise model?}
%Another important step is to evaluate the performance of our approach
%in case of a model mismatch, meaning a noise which is not
%$\alpha$-stable. To do this w
We now consider other models of impulsive noise: the Middleton Class A
\cite{AD-1977} and $\varepsilon$-contaminated noises
\cite{Alhussein2017} We keep the proposed LLR approximation $L_{a,b}$
and test it under three configurations:
\begin{itemize}
\item Highly impulsive $\varepsilon$-contaminated with \(\varepsilon\)
  = $10\%$ and $K=10$, \figurename~\ref{fig:15},
\item Poorly impulsive Middleton class A with $A = 0.01$ and
  \(\Gamma=0.01\) taken from \cite{NF-2009}, \figurename~\ref{fig:16},
\item Highly impulsive Middleton class A with $A = 0.1$ and
  \(\Gamma=0.1\) taken from \cite{YDMY-2017},
  \figurename~\ref{fig:17},
\end{itemize}
In that cases, one can compute the noise variance. We thus present all
numerical simulations as a function of the signal-to-noise ratio
$E_b/N_0$. For all cases, we compare the true LLR, obtained via
numerical integration to the LLR approximations under supervised,
unsupervised parameter estimation and Gaussian designed demapper
$L_a$. For each channel set, in the supervised way, a learning
sequence of length 20000 is used to optimize $\theta$.
\begin{figure}[htbp]
  \centering \includegraphics[width=1.0\linewidth]{fig-15}
  \caption{BER comparison as a function of $E_b/N_0$ between the
    supervised, unsupervised, Gaussian designed LLR approximations and
    the LLR obtained by numerical integration, in highly impulsive
    $\varepsilon$-Contaminated noise with $\varepsilon=0.1$ and
    $K=10$.}
  \label{fig:15}
\end{figure}
\begin{figure}[htbp]
  \centering \includegraphics[width=1.0\linewidth]{fig-16}
  \caption{BER comparison as a function of $E_b/N_0$ between the
    supervised, unsupervised, Gaussian designed LLR approximations and
    the LLR obtained by numerical integration, in poorly impulsive
    Middleton Class A noise with $ A=0.01$ and $\Gamma=0.01$.}
  \label{fig:16}
\end{figure}
\begin{figure}[htbp]
  \centering \includegraphics[width=1.0\linewidth]{fig-17}
  \caption{BER comparison as a function of $E_b/N_0$ between the
    supervised, unsupervised, Gaussian designed LLR approximations and
    the LLR obtained by numerical integration, in highly impulsive
    Middleton Class A noise with $A=0.1$ and $\Gamma=0.1$.}
  \label{fig:17}
\end{figure}

The high robustness of our demapper can be seen through the close
performance obtained between the blind and supervised case from one
side, and between both approximations to the true LLR from the other
side in spite of the change of the type of noise.

\subsubsection{Additive Gaussian noise channel}

We also want to check if the performance of our decoding scheme is not
degraded in a purely Gaussian noise. The approximated LLR $L_{a,b}$ is
tested in the presence of a Gaussian noise and results are shon in
\figurename~\ref{fig:18}.
\begin{figure}[htbp]
  \centering \includegraphics[width=1.036\linewidth]{fig-18}
  \caption{BER comparison as a function of $E_b/N_0$ between the
    supervised, unsupervised LLR approximations and the optimal LLR.}
  \label{fig:18}
\end{figure}

The results show that the proposed approach is still efficient in a
Gaussian environment.

\subsubsection{Analysis}

These numerical simulations illustrate the universality of the
approach. The LLR family has to be wide enough to be able to represent
the linear behavior of exponential-tail noises like the Gaussian and
the non-linear behavior of sub-exponential distributions of the
impulsive noises. The estimation of the LLR approximation parameter
rely on an information theory criteria which does not depend on any
noise assumption nor function to be estimated. As long as the
approximation family ensures the convexity of the optimization
problem, a solution will exist and will be efficient in the
communication context.

The gap between the blind optimization and the true LLR is of the
order of 0.3 dB in the worst case, showing the strength of the blind
optimized demapper. Moreover, one can achieve an enormous gap within
the range [9.5-15] dB between $L_{ab}$ and $L_a$, showing the
influence of handling correctly the impulses that arise due to the
presence of interference. Moreover, our demapper function does not
impact the performance if you do not have impulsive noise so that we
do not need a detection step to distinguish between Gaussian or
impulsive noise.

\section{Conclusion}
\label{section:conclusion}

We propose in this paper a Universal receiver design. We use an LLR
approximation function $f_{\theta}$ in a parametric family. The
parameters $\theta$ are estimated through the maximization of the
mutual information. A blind solution is propose in order to benefit
form the whole received sequence but also to increase the useful data
rate.  Our results show that the receiver design is efficient in a
large variety of noises and that the blind estimation allows to reach
performance close to the optimal and better than the supervised
approach if a reasonable training sequence length is considered.

\section*{Acknowledgment}\label{section:Ack}
This work has been partially supported by COST ACTION CA 15104 -
IRACON, and IRCICA USR CNRS 3380 \bibliographystyle{IEEEtran}
\bibliography{ReviewedPapers}

\end{document}